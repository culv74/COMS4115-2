# Import necessary classes from lexer.py and parser.py
from lexer import Lexer
from parser import Parser

# Sample source code to test the lexer and parser
source_code = """
draw(sun + dog);
"""

# Step 1: Tokenize the source code using the Lexer
lexer = Lexer(source_code)  # Initialize lexer with the source code
tokens = lexer.tokenize()  # Tokenize the source code

# Step 2: Print out the tokens generated by the lexer (optional step for debugging)
print("Tokens Generated by Lexer:")
for token in tokens:
    print(f"<{token[0]}, {token[1]}>")

# Step 3: Parse the tokens using the Parser to generate an AST
parser = Parser(tokens)  # Initialize parser with the tokenized input
ast = parser.parse_program()  # Generate AST from the tokens

# Step 4: Print the generated AST (ensure __str__ or __repr__ methods are implemented in AST)
print("\nGenerated AST:")
print(ast)

